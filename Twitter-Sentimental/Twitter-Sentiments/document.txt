Twitter sentiment analysis is one of the famous projects nowadays and the obvious reason is - Marketting and User behaviour towards the subject

Task 1
In this task, I have designed a bot which will run every midnight and get the top 10 trending hastags of a location (country/state). 

Technical Aspect:
1. Getting Twitter API to get the tweets
2. Defining locations for which I wish to get trending hashtags. As I only have limited requests/hour I am using only few locations. 
3. Getting WOEID (Where On Earth IDentifier) for the locations. Twitter api needs WOEID for getting trending topics from that location. (Using library Tweepy for interating with Twitter API)
4. Fetching top 1000 english tweets for a trending hashtag and saving in the file with today's time as filename. (Folder: <current_directory>/trending_tweets)
5. There are 2 filenames, one contain only trending hashtags and second contains tweets
6. Using (schedule) library, I am making this program a cron job which will run everyday midnight

Important Aspects:
1. If total twitter requests exceeds 100/hour then my program will wait for 1 hour to replenish the requests.
2. As it is a bot, it should only be started once and then it will run forever.
3. For simplicity, I am interested in tweets which has only english alphabets.


Task 2
In this task, I have designed a script which will predict if the hashtag is Positive, Negative or Neutral.

Technical Aspect:
1. Loading dataset from disk, renaming them as per user's preference and deleting unuseful features
2. Tweet data is usually in a bad format. Preprocessing the tweet by converting to Lower case, removing urls, removing @users and #, removing puntuations, removing stopwords from nltk library. 
3. After finding the useful texts from the tweet, we need to convert it into numerical form. There are plenty of techniques which can be used for making a vector of a text like "Bag of words", "tf-idf" etc. For this task I am using tf-idf. My previous expereince with tf-idf is good and hence I am using it for the task. 
4. I have dataset of more than 1 crore labelled tweets with 0 as negative, 2 as neutral, 4 as positive sentiment. Dataset does not have any problems like null value, imbalance etc. Therefore I am using normal train test set splitting with ratio 80:20.
5. For this task I am only using "text" of the tweet as important feature. 
6. Machine Learning algorithm consideration
	- Multinomial Naive Bayes-> Instead of Guassian distribution, this algorithm uses Multinomial distribution of each feature. This works well for data which can easily be turned into counts, such as word 		  counts in text. In our case text feature vector. It is a generative model approach which models the joint distribution of the feature and target, and then predicts the posterior probability. 
		Accuracy achieved: 76%
	- Logistics Regression-> It is one of the popular algorithm for classification. It is a discrimative approach which directly models the posterior probability by learning the input to output mapping by 		  minimising the error.
		Accuracy achieved: 79%
	- I have used SVM also but then I removed it, it was taking a lot of time to learn a model on this data. 
	- When we have a loads of data, which in our cases is lakhs. In addition to it, out feature vector is normalized. We can also use Neural network for more better predictions as it will repeatedly learn 		  the weights and biases by reiterating over the data. 
7. I used the dataset which was generated by the Twitter bot and classify Hashtags in Positive, Negative and Neutral.
	Challenge faced-> While converting text into feature vector, we need to think of the corpus we used for making a vector as the dimension will be then fixed by it. I was getting error because I used 		training data as my corpus and for bot generated test data I used different vector object which generated different dimension. I then changed the architecture a bit and it then worked well.  
8. As for one hashtag there are plenty of tweets, to show result, I grouped the whole prediction and used Max as a aggregation. 

Important Aspects:
1. I have implemented the Porter stemming. The operation is itself computational intensive and also model's accuracy is not improved. Usually in text analysis we want find the usueful meaning of the word,
because stemming destroys the meaning of the context. 
2. Considering only english tweets/text.
3. Considering only text feature of the dataset.
